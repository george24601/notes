Classical data mining: look at disk, bring part of data into memory => Not
enough to handle web load

Cluster Architecture: 1G swtich between nodes in a rack, 2-10 G backbone
switch between racks

Problem: 
node failure is normal, how to store data persistently and keep it
available? => store data redundantly
How to handle node failure in the middle of long computation
Network becomes bottleneck => move computation close to data to minimize
movement

Redundant Storage Infrastructure: Distributed file system => global file
namespace, redundancy, and availability.
Common pattern is huge file > 100 G. Rarely update in place, reads and appends
are common

Data kept in chunks and spread acorss machines, each chunk is replicated on
different machines(chuck servers), which is also the compute server

Chuck servers:
File is in contiguous chunks (16-64 MB). Chuck is replicated 2x or 3x and kept
in different racks(not machines in same rack)

Master node/Name node:
Stores metadata about where files are stored. will be replicated

Client access library:
talk to master to find chuck servers, and then connects directly to chuck
server

-------------

Scenario: huge text document, calculate number times each distinct word
appears in the file

Case 1: file too large, but full dict fits in memory
Case 2: too many words to fit all entries in memory

words(txt) | sort | unique -c

input k-v values => intermediate k'-v' pairs => group by key => reduce a
k'-v's pairs to k'-single v pair

map will be on each node. System will copy map results of same key into a
single node and do final reduce,e.g., via hashing. 

Each map/sort/reduce step tries to use as much sequential scan instead of random access of disk

pseudo code

map(key, value)://key doc name, value:text of doc
	for each word w in value
		emit(w, 1)

reduce (key, values)
	result = 0
	for each count v in values
		result += v
	emit(key, values)

Example:
each record of the form (URL, size,date..), find total number of bytes on HOST

-------
map => reduce are distributed in parallel by partitioning function/hash
function

The framework needs to:
partitioning the input
scheduling execution across machines
group by key
handling failure
inter-machine communication

Scheduler schedule map tasks close to physical storage location of input data
intermediate resutls are on local FS of map/reduce workers (NOT on DFS, which
is the case for input/findal output)

Master node coorndate: task status, idel, in-progress, completed

When a map task completes, sends the master the location and sizes of its R
intermediate files, one for each reducer. Master pushes this info to reducers

Master pings workers periodically to detect failures => 
set map tasks at that worker are reset to idle. 
set in-progress reduce tasks at that worker to idel
abort task if master failed

Make # of map tasks much larger than # of nodes in the cluster. Often one DFS chuck per map
R is much smaller than M

-----
Combiner: pre-aggregating values in the mapper. Combiner is ususally same as
the reduce function. Provided by the programmer.
This requires functions asscociatative and commutative

example: combiner for average function 
combiner for median function?


------------

Two principal costs:
Computation cost of mappers, reducers, and system => sort k-v pairs by key and merging them at Reduce tasks
Communication cost of shipping k-v paris from mappers to reducers

Computation at the mapper is usually small, but system cost of sorting is typically porportional to the communication cost, which donimates cost

high parallelism pushing the communicaiton higher than you'd like for min cost

In many cases, the big issue is wheter a reducer has too much input to operate in main memory: the cost function is stepped when communication gets too low
---------
Example: 3000 drugs, 1M of data per drug, find drug interactions

naive approach: key: pair (i, j), value: record (R(i), R(j))

Performance: 3000 * 3000/2 pairs, each k-v pair has 1 Mb data over 1Gb network

Solution: use fewer keys with longer list of values
group drugs into 30 groups of 100 drugs each=>g(i) = group the drug i goes

map key: set of 2 group members, so mapper for drug i gives 29 key-value pairs
value: pair (i, megabyte-long record for drug i)

the reducer gets that key and a list of 200 records => drugs belongs to m and n, and it will compare each record from group m with each record from group n.

special case: key (i, i)

Each pair is compared at exactly one reducer

instead of 3000 * 3000 records we send over network, we have 3000 * 29 records send instead

-----------
Model for m-r algorithms:

inputs: drug numbers and records
outputs: one output for each pair of drugs

A many-many relationship between in and out

In drug case, output pair is related to both i and j

In matrix multiplication case, (i, j) is related to row i and col j

e.g. join:
Input: all R(a,b) and all R(b, c)
output: all (a,b,c)

how to solve it?

Reducer size q: max # of inputs that a given reducer can have
Replication rate r: number of k-v pairs created by each mapper => represents communication cost

The requirement that there be a mapping shcema is what distinguishs map-reduce algos from general parallel algo

-----------
matrix multiplication
If a reducer covers (i,k) and (f,g), then it also covers (i,g) and (f, k) => each reducer covers all the outputs in the rectangle defined by a set of rows and a set of cols of matrix C

If a reducer gets q inputs, it gets q/n rows or cols => to max # outputs, make the input "square" => q/2n rows and q/2n cols and sqaure of that output

total outputs = n^2 => so need at least 4*n^4/q^2 reducers => 2 n^2 total inputs gives 2n^2/q replication rate

so algo:
divide rows/cols of the first matrix into n/g rows each, so q = 2n^2/g, # of reducers =g^2 based on the result above
r = g 

Two-job approach: first divide both inputs into rectangles, and sum the partial sum
divide rows of A into g groups, cols into 2g groups, cols of B into g groups, rows into 2g group, note that for groups of columns for A and rows for B must have indices that match

Reducer correspond to an n/g * n/2g rectangle in A and an n/2g * n/g matrix in B => i.e., we can identify reducer with (i,j,k) => result is a n/g * n/g matrix, j times the result will go into this matrix, i.e., 2g reducers contribute

One job method communication: 2n^2/q * 2n^2
Two job method: job 2: 2n^2/g
job 1:  2n^2 *g k-v pairs => total


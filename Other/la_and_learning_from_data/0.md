goal : To construct a function that classifies the training data correctly, so it can generalize to unseen test data.

The inputs to the function F are vectors or matrices or sometimes tensors-one input v for each training sample. For the problem of identifying handwritten digits, each input sample will be an image-a matrix of pixels. We aim to classify each of those images as a number from 0 to 9. Those ten numbers are the possible outputs from the learning function. In this example, the function F learns what to look for in classifying the images.

By assigning weights to different pixels in the image, we create the function. Â· The big problem of optimization (the heart of the calculation) is to choose weights so that the function assigns the correct output 0, 1, 2, 3, 4, 5, 6, 7, 8, or 9. And we don't ask for perfection ! (One of the dangers in deep learning is overfitting the data,)

Then we validate the function by choosing unseen MNIST samples, and applying the function to classify this test data. Competitions over the years have led to major improvements in the test results. Convolutional nets now go below 1% errors. In fact it is competitions on known data like MNIST that have brought big improvements in the structure of F. That structure is based on the architecture of an underlyi~g neural net

The inputs are the samples v, the outputs are the computed classifications w = F(v). The simplest learning function would be linear: w = Av. The entries in the matrix A are the weights to be learned : not too difficult. Frequently the function also learns a bias vector b, so that F(v) = Av +b. This function is "affine". Affine functions can be quickly learned, but by themselves they are too simple.

Nonlinearity would come by squaring the components of the input vector v. That step might help to separate a circle from a point inside-which linear functions cannot do. But the construction ofF moved toward "sigmoidal functions" with S-shaped graphs. It is remarkable that big progress came by inserting these standard nonlinear S-shaped functions between matrices A and B to produce A(S(Bv)). Eventually it was discovered that the smoothly curved logistic functions S could be replaced by the extremely simple ramp function now called ReLU(x) =max (0, x).

The functions that yield deep learning have the form F(v) = L(R(L(R(... (Lv))))). This is a composition of affine functions Lv = Av + b with nonlinear functions R - which act on each component of the vector Lv. The matrices A and the bias vectors b are the weights in the learning function F. It is the A's and b's that must be learned from the training data, so that the outputs F(v) will be (nearly) correct. Then F can be applied to new samples from the same population. If the weights (A's and b's) are well chosen, the outputs F(v) from the unseen test data should be accurate. More layers in the function F will typically produce more accuracy in F(v ).

Properly speaking, F(x, v) depends on the input v and the weights x (all the A's and b's). The outputs v1 = ReLU(A1v +b1) from the first step produce the first hidden layer in our neural net. The complete net starts with the input layer v and ends with the output layer w = F(v ). The affine part Lk(vk-l) = Akvk-l + bk of each step uses the computed weights Ak and bk.

Choose weights Ak and bk to minimize the total loss over all training samples.


Choose weights Ak and bk to minimize the total loss over all training samples.
The total loss is the sum of individual losses on each sample. The loss function for least squares has the familiar form IIF(v) - true outputW. Often least squares is not the best loss function for deep learning.

The hidden layers add depth to the network. It is that depth which has allowed the composite function F to be so successful in deep learning. In fact the number of weights Aij and bj in the neural net is often larger than the number of inputs from the training samples v.
This is a feed-forward fully connected network. For images, a convolutional neural net (CNN) is often appropriate and weights are shared-the diagonals of the matrices A are constant. 

The weights in the learning function go into matrices. Those weights are optimized by "stochastic gradient descent". That word stochastic (= random) is a signal that success is governed by probability not certainty. The law of large numbers extends to the law of large functions : If the architecture is well designed and the parameters are well computed, there is a high probability of success.


In my experience, positive definite symmetric matrices S are the aces.

Then F is applied to test data, drawn from the same population as the training data. IfF learned what it needs (without overfitting: we don't want to fit 100 points by 99th degree polynomials), the test error will also be low. 

The function F is often piecewise linear-the weights go into matrix multiplications. Every neuron on every hidden layer also has a nonlinear "activation function". The ramp function ReLU(x) = (maximum of 0 and x) is now the overwhelming favorite.


There is a growing world of expertise in designing the layers that make up F(x, v ). We start with fully connected layers-all neurons on layer n connected to all neurons on layer n + 1. Often CNN's are better-Convolutional neural nets repeat the same weights around all pixels in an image: a very important construction. Other layers are different. A pooling layer reduces the dimension. Dropout randomly leaves out neurons. Batch normalization resets the mean and variance. All these steps create a function that closely matches the training data. Then F(x, v) is ready to use.








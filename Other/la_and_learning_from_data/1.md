The eigenvalue equation Ax = .Ax is very different. There is no vector b-we are looking only at the matrix A. We want eigenvector directions so that Ax keeps the same direction as x. Then along that line all the complicated interconnections of A have gone away. The vector A2x is just .A2x.

A basis for a subspace is a full set of independent vectors : All vectors in the space are combinations of the basis vectors.

The rank of a matrix is the dimension of its column space.


Then A = CR stores this information as a matrix multiplication. Actually R is a famous matrix in linear algebra:

R = rref(A) = row-reduced echelon form of A (without zero rows).

The number of independent columns equals the number of independent rows

The big factorization for data science is the "SVD" of A-when the first factor C
has r orthogonal columns and the second factor R has r orthogonal rows.

Row rank = Column rank r independent columns {::} r independent rows


Why is the outer product approach essential in data science ? The short answer is : We are looking for the important part of a matrix A. We don't usually want the biggest number in A (though that could be important). What we want more is the largest piece of A. And those pieces are rank one matrices u v T . A dominant theme in applied linear algebra is :
Factor A into CR and look at the pieces ckr~ of A= CR

Symmetric matrix S 

Orthogonal matrix Q 3





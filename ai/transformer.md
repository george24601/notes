### Attention(Q, K, V) 

Input embedding is transformed into 3 vectors
* Q is what a word is looking for
* K is how much a word offers
* V is what a word communicates, or a word’s actual content

The cat sat on the mat because it is tired. Query is “it”, 

* GPT is decoder only
QKV are trained implicitly through back propagation

